% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[11pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm2e}


\usepackage{tcolorbox}

 
\newtheorem{theorem}{Satz}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Beweis]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Numerical Optimization - Summary}%replace X with the appropriate number
\author{Andy Tr√†n}
 
\maketitle %shows the current date of the Zusammenfassung

This will be a very personalised summary for me to use to study for the course Numerical Optimization (AIST 3010). It might be complete, it might not be, it will probably not be. For questions you can refer to \href{mailto:andtran@ethz.ch}{andtran@ethz.ch}. This summary is based of the lecture notes and should be used as a supplement to the lectures. 

\tableofcontents

\newpage

\section{Lecture 1 - 05/09/2022: Introduction to Optimization}
\begin{itemize}
    \item \textbf{Main lecture}: Monday 12:30 - 2:15, Wednesday 5:30 -6:15 (only ESTR3112)
    \item \textbf{Tutorial lecture}: Wednesday 4:30-5:15
    \item \textbf{Prereq}: Multivariable calculus, linear algebra
    \item \textbf{Course materials:} Homework and exam questions solely based on lecture notes.
    \item \textbf{Email}: farnia@cse.cuhk.edu.hk
    \item \textbf{Office Hour}: Tuesday 2-3 pm, SHB Building, Office 918
    \item \textbf{Learning goals:}  \begin{enumerate}
        \item Formulating optimization problems belonging to standard optimization categories for engineering and AI tasks
        \item Applying standard optimization algorithms to solve linear and convex programming tasks
        \item Implementing standard optimization algorithms over Python
    \end{enumerate}
    \item \textbf{Grading}: Homework 0.20, midterm 0.30, final .50, participation additionally 0.05 
\end{itemize}

\section{Tutorial 1 - 07/09/2022: Introduction to Optimization}
\begin{example}[Transportation problem] we want to minimize the total cost of transporting a commodity from m factories to n stores. We have to following constraints: 
    \begin{itemize}
        \item factory i can supply at most $a_i$ units of the commodity
        \item store j needs at least $b_j$ units of commodity
        \item the cost of shipping from factory i to store j is $c_{i,j}$ per unit   
    \end{itemize}

    We get the following system

    \begin{itemize}
        \item Optimization variables: $x_{i,j}$, the amount of units from fac i to store j
        \item Objective function: $\sum_{i,j}c_{i,j}x_{i,j}$
        \item Constraint function: $\sum_{i}x_{i,j} \leq a_i$, $\sum_{j}x_{i,j} \geq b_j,$, $x_{i,j}\geq 0 $   
    \end{itemize}
\end{example}
For above problem, there is no analytical solution, only an interative solution.

\begin{example}[Manufacturing task] we want to maximize the profit of producing n products from m raw materials, given that
    \begin{itemize}
        \item We have a profit of $c_i$ per unit of Product i
        \item We have $b_j$ available units of raw material j
        \item We need $a_{i,j}$ units of raw material j for manufacturing one unit of i
    \end{itemize}
    We get the following system
    \begin{itemize}
        \item Optimization variable: $x_i$, amount of units per product i
        \item Objective function: $\sum_{i}c_i x_i$
        \item Constraint function: $\sum_i x_i a_{i,j} \leq b_j$ for all j   
    \end{itemize}
\end{example}
Obviously you have to define what the allowed values are for i and j, which is left as an exercise to the reader.

\begin{example}[Sorting task] given real numbers $c_1, ..., c_n \in \mathbb{R}$ we want to find the k smallest numbers
\begin{itemize}
    \item Optimization variable: For every $1 \leq i \leq n, x_i = \begin{cases}
        1 & \text{if $c_i$ is among the smallest k} \\
        0 & \text{otherwise}
    \end{cases}$ 
    \item Objective function: $\sum_i^n = x_i c_i$
    \item Constraint function: $\sum_i^n x_i = k$, $x_i(1-x_i) = 0$ for all i  
    
\end{itemize} 
    
\end{example}

\section{Tutorial 2 -  14/09/2022: Vectors}
\subsection*{Vectors}
A vector $ x= [x_1, ..., x_n]$ is a collection of numbers, arranged in a column or a row, which can be thought of as the coordinates of a point in a n-dimensional space. 
\begin{itemize}
    \item Addition: is defined elementwise, given the dimensions are the same
    \item Scalar product: element wise multiplication with a scalar.
\end{itemize}

Note: we by default assume a vector follows a column-representation, with real values. For row format we can just transpose. 

\begin{definition}[Linearly independent]
    Given we have n vectors, we call them l.i. when $c_1 x_1 + ... + c_n x_n = 0$ only has one solution, namely all $c_1, ..., c_n = 0$
\end{definition}

\begin{definition}[Basis]
    For a subspace $S \in \mathbb{R}^d$, is a set of l.i. vectors $B = [x_1, ..., x_m]$ such that every vector $x \in S$ is a linear combination of the vectors in B.   \newline
    Standardbasis: defined where $0 \leq i \leq m$, where i'th coordinate of $e_i \in S_b$ is 1 and all the others 0.  
\end{definition}

\begin{definition}[Euclidean length]
    $x : = \sqrt{x_1^2 + ... + x_n^2}$ 
\end{definition}

\begin{definition}[Norm function] Properties of a norm function
    \begin{enumerate}
        \item $||x|| \geq 0$, equal to 0 only when $x = 0$
        \item For every $c\in \mathbb{R}, ||cx|| = |c|||x||$
        \item For every $x, y \in \mathbb{R}^d, ||x+y|| \leq ||x|| + ||y||$    
    \end{enumerate}
    
    
\end{definition}

\begin{definition}[$l_p$-norm ]
    For every $ p \geq 1 $ we define $l_p$ norm $|| \cdot ||_p : \mathbb{R}^d \to \mathbb{R}$ as $||x||_p = (|x_1|^p + ... |x_n|^p)^\frac{1}{p}$    
    
\end{definition}
Note: $l_\infty = \underset{1 \leq i \leq d}{\max}x_i$ 
\newline
Theorem: $l_p-norms$ are decreasing in $ p \geq 1$, so
\[
1 \leq p \leq q \leq \infty \Rightarrow ||x||_p \geq ||x||_q
.\]   

\begin{definition}[Inner Product]
    For every $ x = [x_1, ..., x_n], y = [y_1, ..., y_n ]$ we define their inner product $<x, y> = \sum_i^n x_i y_i = x^ty$  
\end{definition}

\section{Lecture 2, Part 2 - 19/09/2022: Vectors and Matrices}
\begin{theorem}Cauchy-Schwarz Inequality
    For two vectors $\textbf{x} $ and $\textbf{y} $, the following inequality holds:
    \[
    |<\textbf{x} ,\textbf{y} >| \leq ||\textbf{x} ||||\textbf{y} ||
    .\]    
\end{theorem}

\begin{theorem}Angle between two vectors
    We can calculate the angle between to vectors namely, \[
    \cos \theta = \frac{<\textbf{x},\textbf{y}>}{||\textbf{x}||||\textbf{y} || }
    .\] 
    
\end{theorem}
They are orthogonal when the scalar product is 0, they are aligned the same or opposite position when the angle is 0 or 180 degrees.

\begin{definition}[Mutual Orthogonal Vectors] a group of vectos $x^{(1)}, ..., x^{(k)}$ 
    For all $i \neq j: <x^{(i)}, x^{(j)>} = 0$ 
    
\end{definition}

\begin{proposition}[Mutual Orthogonality implies linear independence]
    A set of mutually orthogonal vectors are linearly independent
    
\end{proposition}

A collection of vectors are called orthogonal if they have unit euclidean length and are mutually orthogonal\dots

%TODO missed something page 8

\subsection*{Vectors in Optimization problems}
From the future on we write an optimization problem as follows:
\[
\underset{\textbf{x} }{min} f(\textbf{x}
\newline \text{subject to}
g_1(\textbf{x} ) \leq 0 
\newline
\dots
\newline
g_n(\textbf{x} ) \leq 0 
.\] 

\subsection*{Matrix}
A matrix $A \in \mathbb{R}^{m\times n}$ is a two-dimensional array of numbers
$\begin{bmatrix} a_{1,1} & a_{2,1} & \dots & a_{m,1}\\
    a_{1,2} & a_{2,2} & \dots & a_{m,2}\\
    . & . & . \\
    a_{1,n} & a_{2,n} & \dots & a_{m,n}

\end{bmatrix} $  

The product of two matrix $A \in \mathbb{R}^{m\times n}, B \in \mathbb{R}^{n\times t}$ is defined as
\[
[AB]_{i,j} = \sum_{k=1}^{n} a_{i,k}b_{k,j}
.\]  

\begin{itemize}
    \item \textbf{The identity matrix} $I_{n}$  is defined as an $n \times n$ matrix with the diagonal being 1 and all other elements being 0.
    \item \textbf{Matrix vector product} is the same as matrix multiplication, but t being 1
    \item \textbf{Range of Matrix}: $\mathcal{R}(A)$ the subspace of all vectors following from linear combinations of A's combinations
    \item \textbf{Rank of matrix}: $Rank(A)$ the dimension of subspace $\mathcal{R}(A)$  
    \item \textbf{Full-rank matrix}: $Rank(A_{m\times n}) \leq min(m,n)$ and a matrix is full rank if $Rank(A_{m\times n}) = min(m,n)$
    \item \textbf{Null-space}: $\mathcal{N}(A)$: the subspace of all vectors which A maps to 0. $\{\textbf{x}: A\textbf{x} = \textbf{0} \} $  
    \item \textbf{Determinant}: $det(A) = \sum_{j}^{n} (-1)^{i+j}a_{i,j}det(A_{(i,j)})$  
\end{itemize}

Rules about determinant for square matrices:
$det(A) = \begin{cases}
    n, & \text{when regular}\\
    0, & \text{otherwise}
\end{cases}$

\begin{definition}[Invertible matrices] we call a square matrix $A_{n\times n}$ invertible when a matrix exists such that 
    \[
    A^{-1}A = I_n \text{or} AA^{-1}
    .\]    holds
    
\end{definition}

\begin{proposition}[Non-singular matrices] Following propositions are equivalent.
    \begin{enumerate}
        \item A is invertible
        \item A is non-singular, i.e $det(A)\neq 0$
        \item A is full rank, i.e $Rank(A) = n$
        \item A has linearly  independent rows or columns
        \item A has zero null subspace
        \item A has %TODO page 16  
    \end{enumerate}
\end{proposition}

\subsection*{Basic identities}
\begin{itemize}
    \item $(A^T)^{-1} = (A^{-1})^T$
    \item $(AB)^T = B^TA^T$
    \item $(AB)^{-1} = B^{-1}A^{-1}$
    \item $det(A^T) = det(A)$
    \item $det(A^{-1}) = \frac{1}{det(A)}$
    \item empty %TODO     
\end{itemize}

\subsection*{Eigenvalues and Eigenvectors}
\begin{definition}[Eigenvector and eigenvalues]
    We call a non-zero vector $\textbf{v} \neq \textbf{0}$ an eigenvector of a matrix $A_{n\times n}$ if for a coefficient $\lambda$
    \[
    A\textbf{v} = \lambda\textbf{v}  
    .\]    
\end{definition}
\begin{enumerate}
    \item Eigenvalues can be characterized as the solutions for $det(A- \lambda I) = 0$
    \item Every $A\in \mathbb{R}^{n\times n}$ has n eigenvalues $\lambda_1, ..., \lambda_n$ counting multiplicities
    \item All eigenvalues of a symmetric matrix are real numbers and can be sorted as $\lambda_1 \geq ... \geq \lambda_n$    
\end{enumerate}

\begin{proposition}[Determinant and eigenvalues]
    $det(A) = \lambda_1 \lambda_2 ... \lambda_n$ and so A is invertible $\Leftrightarrow$ all its eigenvalues are non-zero.  
\end{proposition}

\begin{proposition}[Eigenvectors of symmetric matrices]
    Supposed $v_1$ and $v_2$ are two eigenvectors of a symmetric matri xA for different eigenvalues $\lambda_1 \neq \lambda_2$ then
    \[
    v_1 \perp v_2 \text{ i.e. } v_1^T v_2 = 0
    .\]    
    
\end{proposition}

\begin{theorem}[Spectral theoren]
    For a symmetric matrix A, the set of mutual orthogonal eigenvectors $v_1, ..., v_n$ corresponding to A's eigenvalues $\lambda_1, ..., \lambda_n$. In addition if $v_1, ..., v_n$ are normalized and have unit norm, then
    \[
    A = \sum_{i=1}^{n} \lambda_i v_i v_i^T = V \Lambda V^T  
    .\], where $V = [v_1 | v_2 | ... | v_n]$, $\Lambda = diag(\lambda_1, ..., \lambda_n)$ 
    
\end{theorem}

\begin{example}Spectral decomposition for the following matrix
    $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, calculating $det(A -\lambda I) \overset{!}{=} \lambda_1 = -1, \lambda_2 = 3$ and the eigenvectors result in $v_1 = [\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}]$, $v_2 = [\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}]$   
\end{example}

\begin{definition}[Positive semi-definite matrix]
    A symmetric matrix A is called positive semi-definite (PSD) if for every vector $\textbf{x} \in \mathbb{R}^n $ we have 
    \[
    x^t A x \geq 0
    .\]
    positive definite when the above equality holds strictly for $\textbf{x}  \neq 0$ 
    
\end{definition}

\begin{theorem}[Eigenvalues and PSD matrix] Following propositions are equivalent
    \begin{itemize}
        \item A is positive semi-definite
        \item A's eigenvalues are all non-negative
        \item For a matrix H we have $ A = H H^T$ 
    \end{itemize}
\end{theorem}

\subsection*{Ordering positive smei-definite matrices}
\begin{definition}[Partial order for Matrices] for two square matrices A, B we say
    \begin{itemize}
        \item $A\succeq B$ if A - B is a positive semidefinite matrix
        \item $A \succ B$ if A - B is a positive definite matrix
        \item $A \preceq B$ if A - B is a negative definite matrix
        \item $A \prec B $ if A - B is a negative definite matrix 
    \end{itemize}
    Note what happens when $B = 0$ and note that $ A\nsucceq B $ does not imply $A \prec B$ %TODO page 23   
    
    
\end{definition}

\section{Lecture 3 - 26/09/2022: Multivariable Functions and Calculus} %TODO: give a name
%TODO: didnt pay attention start from page 6

\subsection*{Graphs and Epigraphs}
\begin{itemize}
    \item Consider a real-valued function $f:\mathbb{R}^n \to \mathbb{R}$, 
    \item Graph: The set of all points $(x, f(x)) \in \mathbb{R}^{n+1}$, graph $f:=\{(x,f(x)):x\in\mathbb{R}^n \}$
    \item Epigraph: Set of points on top of f's Graph, $epi f := \{(x,t):x\in\mathbb{R}^n, t \geq f(x)\}$
    \item Example $f([x_1,x_2]) = x_1^2 +x_2^2$  
\end{itemize}


\subsection*{Contour set}
\begin{definition}[Contour Set]
    For a real value $t\in \mathbb{R}$ the \textbf{contour} of f is the set of vectors mapped to value t.
    \[
    C_f(t) = \{ \mathbf{x} \in \mathbb{R}^n: f(\mathbf{x}) = t \}
    .\]  
\end{definition}

 \subsection*{Linear and affine functions}
 $f: \mathbb{R}^n \to \mathbb{R}$ is called a linear function if 
 \begin{enumerate}
    \item For every $x,y \in \mathbb{R}^n: $ $f(x+y) = f(x) + f(y)$ 
    \item For every $x \in \mathbb{R}^n, c\in \mathbb{R}: $ $f(cx) = c f(x)$  
    \item $f(0) = 0$, which follows from point (2).
    \item We call $f$ an affine function if $g(x) = f(x) - f(0)$ is a linear function   
 \end{enumerate}
  
 \begin{theorem}[Linear functons and inner products]
    Suppose that f is a linear function, then there exists a vector $a\in\mathbb{R}^n$ such that:
    \[
    f(\mathbf{x}) = \mathbf{a^Tx}= a_1x_1 + ... + a_nx_n
    .\]  
    
 \end{theorem}

 %TODO: missed something on  page 9

 \subsection*{Hyperplane and Halfspaces}
 \begin{itemize}
    \item \textbf{Hyperplane:} a contour of a linear function, that is for a vector $\mathbf{a}\in\mathbb{R}^n$ and real number $b\in \mathbb{R}$ we define a hyperplane H as: \[
    H = \{ \mathbf{x} \in \mathbb{R}^n: \mathbf{a}^T \mathbf{x} = b\}
    .\]    
    \item \textbf{Halfspace:} the space on top or bottom of a hyperplane, that is for a vector $a\in \mathbb{R}^n$ and real number $b\in \mathbb{R}$ we define halfspace $H_-$ and $H_+$ as \[
    H_+ = \{x \in \mathbb{R}^n : a^tx \leq b\}
    .\], conversely for $H_+$ 
    
 \end{itemize}

 \subsection*{Quadratic functions}
 \begin{itemize}
    \item \textbf{Quadratic function: } a polynomial function in which the highest-degree term is of degree 2
 \end{itemize}

 \begin{definition}[General quadratic function] The following provides a general form of a quadratic function in terms of $A\in\mathbb{R}^{n\times n}, b \in \mathbb{R}^n$, and $c\in \mathbb{R}$\[
 f(x) = \frac{1}{2}x^t Ax + b^t x + c
 .\]   
    
 \end{definition}
 
 \subsection*{Gradients}
 \begin{definition}[Gradient] if f is differentiable at a point $x \in \mathbb{R}^n$, we define the gradient as
    \[
    \nabla f(x) = [ \frac{df(x)}{dx_1}, ..., \frac{df(x)}{dx_2}]
    .\]  
    
 \end{definition} %TODO; maybe add gradient rules for vectors and matrices
 
 \subsection*{Gradient used for local function approximation}
 \begin{itemize}
    \item In many optimization problems, we need to locally approximate the objective and constraint functions with an affine function
    \item At every point $x_0$ the gradient $\nabla f(x_0)$ results in an affine function approximating function f around $x_0$
    \item The approximation error is defined as $f(x) = f_{app}(x) + \epsilon(x)$  
 \end{itemize}
 
 \[
 f(x) \approx f(x_0) + \nabla(f(x_0))^T(x-x_0)
 .\] 

 \begin{theorem}[Error of first-order taylor series expansion] Suppose that f is differentiable at $x_0$, then error $\epsilon(x)$ is vanishing near $x_0$
    \[
    lim_{x \to x_0} \frac{\epsilon(x)}{||x-x_0||} = 0
    .\]    
    
 \end{theorem}

 \subsection*{Geometric interpretation of gradients}
 For a unit vector $\mathbf{v}$ and real $\epsilon>0$ \[
f(x_0 + ev) \approx f(x_0) + \epsilon \nabla f(x_0)^T v
.\]  
\begin{itemize}
    \item $\nabla f(x_0)^T v > 0$: increases
    \item $\nabla f(x_0)^T v < 0$: decreases
    \item $\nabla f(x_0)^T v = 0$: doesn't change
\end{itemize}
Therefore we have a maximal rate of local variation along the gradient. In contrast there is a zero rate of variation along any direction orthogonal to the gradient. This shows gradients are orthogonal to contour sets.

\subsection*{Hessian}
%TODO: fuck writing the hessian down.
 

\section{Lecture 5 - 03/10/2022: ..}
\subsection*{Convex sets}
\begin{itemize}
    \item Linear combination of vectors $x_1, ..., x_k$ with scalar coefficients $\theta_1, \theta_k $: $\sum_{i=1}^{k} \theta_i x_i$
    \item Convex combination is a linear combination with coefficients $\theta_1, ..., \theta_k$ that satisfy: $\theta_i \forall i$ and $\sum_{i=1}{k} \theta_i= 1 $   
\end{itemize}

\begin{definition}[Convex set] $S \subseteq \mathbb{R}^d$ is called a convex set if for every $x,y \in S$, S includes the line segment between x and y \[
\text{For all} x,y\in S, \theta \in[0,1]: \theta x (1-\theta)y \in S
.\]   
    
\end{definition}
  
\begin{proposition}
    A convex set S is closed to any convex combination of its points, i.e. for every $k\in \mathbb{N}$, non-negative $\theta_1, ..., \theta_k \geq 0$, $\sum_{i=1}^{k} \theta_i = 1$    
\end{proposition} %TODO: add sth from slide 8


\begin{itemize}
    \item Examples of convex sets \begin{enumerate}
        \item Hyperplanes $\{ \mathbf{x}: \mathbf{a^Tx} = b \}$
        \item Halfspaces $\{ \mathbf{x}: \mathbf{a^Tx} \leq b \}$
        \item Norm balls $\{ \mathbf{x}: \mathbf{||x||} \leq \epsilon \}$ 
    \end{enumerate}

    \item Convexity preserving operations: convex sets are closed to the following operations: \begin{itemize}
        \item Intersection
        \item Affine and inverse-Affine mappings: $f:\mathbb{R}^d \to \mathbb{R}^m, f(x) = Ax + b$ 
        \item Linear fractional functions: $f:\mathbb{R}^d \to \mathbb{R}^m, f(x) = \frac{Ax + b}{c^Tx +d}$, where $dom(f) =$ %TODO: finish slide 9  
    \end{itemize}
    
\end{itemize}

\subsubsection*{Convex functions: }
\begin{theorem}[Affine functions and convex combinations]
    A multivariable function $f:\mathbb{R}^d \to \mathbb{R}$ is an affine function if and only if for all \[
        \mathbf{x,y}\in \mathbb{R}^d, \theta \in [0,1]: f(\theta x + (1-\theta)y = \theta f(x) + (1-\theta)f(y))
    .\]    
    
\end{theorem}

\begin{definition}[Convex functions]
    We call f a convex function if $dom(f)$ is a convex set and $\forall x,y \in dom(f), \theta\in [0,1:] f(\theta x + (1-\theta) y) \leq \theta f(x) + (1-\theta)f(y)$. Also we call f concafe if $-f$ is convex.    
\end{definition}

Observation: an affine function f is both convex and concafe. If a function is both convex and concave then it's also affine.

Examples of scalar functions:
\begin{itemize}
    \item Convex functions \begin{enumerate}
        \item powers: $f(x) = x^p$ for $dom(f) = \mathbb{R}_+, p\geq_1$ or $p\leq0$   
        \item Absolute powers: $f(x) = |x|^p$ for $dom(f) = \mathbb{R}, p \geq 1$  
        \item Exponential: $f(x) = e^{ax+b}$ for $dom(f) \mathbb{R}$, $a,b\in\mathbb{R}$   
    \end{enumerate}
    \item Concave functions: \begin{enumerate}
        \item Powers: $f(x) = x^p$ for $dom(f) = \mathbb{R}_+, 0 \leq p \leq 1$
        \item Logarithm: $f(x) = log(x)$ for $dom(f) = \mathbb{R}_+$    
    \end{enumerate}

\end{itemize}

Examples of multivariable functions:
\begin{itemize}
    \item Convex functions: \begin{enumerate}
        \item Norms: $f(x) = ||x||$ for $dom(x) = \mathbb{R}^d$
        \item Norm powers: $f(x) = ||x||^p$ for $dom(f) = \mathbb{R}^d, p\geq 1$     
        \item Log-sum-exp: $f(x) = log(\\sum_{i=1}^{d}e^{x_i})$ for $dom(f) = \mathbb{R}^d$  
    \end{enumerate}
    \item Concave functions: \begin{enumerate}
        \item Entropy: $\sum_{i=1}^{d} x_i log(\frac{1}{x_i})$ for $dom(f) = \mathbb{R}_+^d$  
        \item Geometric mean: $f(x) = (x_1x_2...x_d)^{\frac{1}{d}}$ for $dom(f) = R_+^d$  
    \end{enumerate}
\end{itemize}

Methods of identifying convex functions:
\begin{enumerate}
    \item Verifying the defining inequality (hard!!)
    \item Epigraph and sub-level sets
    \item Gradients and hessians
    \item Operations that preserve convexity
\end{enumerate}

\subsection*{Epigraph and sub-level sets}
\begin{proposition}[Epigraph of convex functions]
    A function f is convex if and only if its epigraph is a convex set
\end{proposition}

\begin{itemize}
    \item We defined the contour (level) set of $C_f(t)$ as the set of inputs mapped to t: \[
    C_f(t) = \{x \in dom(f): f(x) = t \}
    .\]   
    \item We defined the sublevel set $S_f(t)$ as the set of inputs mapped to below t: \[
    S_f(t) = \{ x \in dom(f) : f(x) \leq t \}
    .\]  
\end{itemize}

\begin{proposition}
    If f is a convex function, $S_f(t)$ will be a convex set for every t. 
\end{proposition}


\subsection*{Gradient and Hessian of convex functions}
\begin{theorem}[First-order convexity condition]
    A differentiable f is convex if and only if its domain is convex and \[
    \forall x,y\in dom(f): f(y)  \geq f(x) + \nabla f(x)^T (y-x)
    .\] 
    
\end{theorem}

We can see the RHS is the first-order taylor series expanion around x, if f is twice differentiable we can use the second-order Taylor expansion

\begin{theorem}[Second-order convexity condition]
    A twice-differentiable f is convex if and only if its domain is convex and \[
    \forall x\in dom(f): H_f(x) \text{is PSD}
    .\] 
    
\end{theorem}

\subsection*{Convex quadratic functions}
\begin{itemize}
    \item Consider a quadratic function characteerized by symmetric A,b,c: \[
    f(x) = \frac{1}{2}x^T Ax + b^Tx + c
    .\] 
    \item We earlier derived the gradient and Hessian as $\nabla f(x) = Ax + b, H_f(x) = A$
    \item Second-order condition: f is convex if and only if A is PSD
    \item Application to the least-squares objective $f(x) = ||Ax+b||^2$  
\end{itemize}

\subsection*{Convexity preserving operations}:
\begin{itemize}
    \item Positive scalar multiplication
    \item Addition of convex functions
    \item Composition with affine functions, i.e. $f(Ax+b)$, if f is convex
    \item Pointwise maximum: $max(f_1(x), ..., f_k(x))$ is convex if each $f_i$ is convex
    \item Composition $f(g(x))$ is convex if f,g are convex and f is non-decreasing in every entry   
\end{itemize}

\section{Lecture 6: ...} %TODO: give it a name


\end{document}